1) losses.py：把硬 clamp 改成 STE（Straight-Through Estimator）clamp

这样前向仍使用 [logv_min, logv_max] 的值（数值安全），但反向梯度直接从未裁剪前的 logv 传回去，不会被卡死。

把 losses.py 顶部改成下面这样（新增 _ste_clamp，并在所有 NLL 里用它替代原来的 torch.clamp）：

from __future__ import annotations
import torch
import torch.nn.functional as F

def _ste_clamp(x: torch.Tensor, lo: float, hi: float) -> torch.Tensor:
    """Forward: clamp，Backward: identity（避免梯度被硬截断）"""
    y = torch.clamp(x, min=lo, max=hi)
    return x + (y - x).detach()

def nll_iso3_e2(e2sum: torch.Tensor, logv: torch.Tensor, mask: torch.Tensor,
                logv_min: float=-16.0, logv_max: float=6.0) -> torch.Tensor:
    if logv.dim() == 3 and logv.size(-1) == 1:
        logv = logv.squeeze(-1)
    if e2sum.dim() == 3 and e2sum.size(-1) == 1:
        e2sum = e2sum.squeeze(-1)
    lv = _ste_clamp(logv, logv_min, logv_max)
    v = torch.exp(lv).clamp_min(1e-12)
    nll = 0.5 * (3.0 * lv + e2sum / v)
    m = mask.float()
    return (nll * m).sum() / torch.clamp(m.sum(), min=1.0)

def nll_iso2_e2(e2sum: torch.Tensor, logv: torch.Tensor, mask: torch.Tensor,
                logv_min: float = -16.0, logv_max: float = 6.0) -> torch.Tensor:
    if logv.dim() == 3 and logv.size(-1) == 1:
        logv = logv.squeeze(-1)
    if e2sum.dim() == 3 and e2sum.size(-1) == 1:
        e2sum = e2sum.squeeze(-1)
    lv = _ste_clamp(logv, logv_min, logv_max)
    v = torch.exp(lv).clamp_min(1e-12)
    m = mask.float()
    nll = 0.5 * (2.0 * lv + e2sum / v)
    return (nll * m).sum() / torch.clamp(m.sum(), min=1.0)

def mse_anchor_1d(logv: torch.Tensor, y_var: torch.Tensor, mask: torch.Tensor, lam: float=1e-3) -> torch.Tensor:
    if logv.dim() == 3 and logv.size(-1) == 1:
        logv = logv.squeeze(-1)
    y = torch.clamp(y_var, min=1e-12).log()
    m = mask.float()
    se = (logv - y)**2 * m
    return lam * se.sum() / torch.clamp(m.sum(), min=1.0)

def nll_diag_axes(e2_axes: torch.Tensor, logv_axes: torch.Tensor, mask_axes: torch.Tensor,
                  logv_min: float=-16.0, logv_max: float=6.0) -> torch.Tensor:
    lv = _ste_clamp(logv_axes, logv_min, logv_max)
    inv_v = torch.exp(-lv)                 # (B,T,3)
    nll = 0.5 * (e2_axes * inv_v + lv)    # (B,T,3)
    m = mask_axes.float()
    num = (nll * m).sum()
    den = torch.clamp(m.sum(), min=1.0)
    return num / den

def nll_diag_axes_weighted(e2_axes: torch.Tensor, logv_axes: torch.Tensor, mask_axes: torch.Tensor,
                           axis_w: torch.Tensor=None,
                           logv_min: float=-16.0, logv_max: float=6.0):
    lv = _ste_clamp(logv_axes, logv_min, logv_max)
    inv_v = torch.exp(-lv)                    # (B,T,3)
    nll_axes = 0.5 * (e2_axes * inv_v + lv)  # (B,T,3)
    m = mask_axes.float()
    num = nll_axes.mul(m).sum(dim=(0,1))      # (3,)
    den = m.sum(dim=(0,1)).clamp_min(1.0)     # (3,)
    per_axis = num / den                       # (3,)
    if axis_w is None:
        axis_w = torch.ones_like(per_axis)
    axis_w = axis_w * (3.0 / axis_w.sum().clamp_min(1e-6))  # 归一（均值=1）
    return (per_axis * axis_w).sum(), per_axis.detach()


这处改动只影响 loss 的反向传播；评估指标里仍然用硬 clamp（保持统计一致）。对应原实现见你项目的 losses.py。

2) train.py：给方差头一个可解释的暖启动 bias（避免一上来就打到底）

在创建完模型后、优化器之前，加下面这段——它用一小批训练数据的均值误差来初始化 head.bias，让初始 var≈E[err²]（GNSS 逐轴），这样一开始 z²≈1，不会被 logv_min 立刻吸死。

在 train.py 里、model = IMURouteModel(...).to(args.device) 之后插入：

# ---- Warm start the head bias with data statistics ----
with torch.no_grad():
    b = next(iter(train_dl))
    b = to_device(b, args.device)
    if args.route == "gns":
        e2_axes = b["E2_AXES"].float()                 # (B,T,3)
        m_axes  = b["MASK_AXES"].float()
        num = (e2_axes * m_axes).sum(dim=(0,1))
        den = m_axes.sum(dim=(0,1)).clamp_min(1.0)
        var0 = (num / den).clamp_min(1e-12)            # (3,)
        model.head.bias.data = var0.log().to(model.head.bias)
    elif args.route == "vis":
        e2 = b["E2"].float().squeeze(-1); m = b["MASK"].float()
        var0 = ((e2 * m).sum() / m.sum() / 2.0).clamp_min(1e-12)  # df=2
        model.head.bias.data.fill_(float(var0.log()))
    else:
        e2 = b["E2"].float().squeeze(-1); m = b["MASK"].float()
        var0 = ((e2 * m).sum() / m.sum() / 3.0).clamp_min(1e-12)  # df=3
        model.head.bias.data.fill_(float(var0.log()))


（放置位置：就在你打印 [model] params=... 那块下面即可。）训练/验证打印与指标逻辑保持原样。